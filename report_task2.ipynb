{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a0ef0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler,RobustScaler\n",
    "from sklearn.pipeline import Pipeline,FeatureUnion\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest,f_regression,mutual_info_regression\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "my_NIA=262603\n",
    "np.random.seed(my_NIA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634a11f8",
   "metadata": {},
   "source": [
    "First of all we are going to download the data neccesary for this project. It is already splitted into training and testin, therefore we assign each of them to a variable which contains each part.\n",
    "\n",
    "Then we select the features that we want to work with and the target variable for both, training and testing.\n",
    "\n",
    "In order to be able to validate the model while training it, we are also going to split this first training data into training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f62cebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('/Users/ignacioalmodovarcardenas/Desktop/Advanced programming/hyper_parameter_tuning/traintestdata_pickle/trainst1ns16.pkl')\n",
    "test = pd.read_pickle('/Users/ignacioalmodovarcardenas/Desktop/Advanced programming/hyper_parameter_tuning/traintestdata_pickle/testst1ns16.pkl')\n",
    "\n",
    "train_target=train.loc[:,\"energy\"]\n",
    "test_target=test.loc[:,\"energy\"]\n",
    "\n",
    "train_train=train.iloc[:,0:300]\n",
    "test_test=test.iloc[:,0:300]\n",
    "\n",
    "#Split into train and train validation\n",
    "train_split=train_train.iloc[:3650,:]\n",
    "train_validation=train_train.iloc[3650:,:]\n",
    "\n",
    "train_target_split=train_target.iloc[:3650]\n",
    "train_target_validation=train_target.iloc[3650:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b805f73",
   "metadata": {},
   "source": [
    "## NA imputation\n",
    "\n",
    "Once we have the different parts of the datasets ready we want to impute missing values in random places of the data. \n",
    "\n",
    "We are going to select a 10% of the columns at random. This columns will be the ones were we are going to impute NAs. To do that we create an index with 30 variables taking values from 0 to 300.\n",
    "\n",
    "With the information in this columns we want to impute a 10% of all the obsservations given. Therefore we generate another random index with that amount of values. \n",
    "\n",
    "To finish the imputation we generate another index with the same dimension as the one with all the observations using only the features generated in the first case.\n",
    "\n",
    "Then using a simple loop we impute each NA in the place generated at random. We repit this process for both test and training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4358610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_NA_train=train_train.copy()\n",
    "test_NA_test=test_test.copy()\n",
    "\n",
    "#Take 30 columns at random\n",
    "index_column_NA=np.random.choice(300,30)\n",
    "\n",
    "#Take the total number of data and calculate its 10%, this will be the columns where we impute NAs\n",
    "nNAs_train=int(index_column_NA.shape[0]*train_NA_train.shape[0]*0.1)\n",
    "nNAs_test=int(index_column_NA.shape[0]*test_NA_test.shape[0]*0.1)\n",
    "\n",
    "#Create a random index for the observations. The dimension will be the number of na neccesary\n",
    "index_rNA_train=np.random.choice(int(train_NA_train.shape[0]),nNAs_train)\n",
    "index_rNA_test=np.random.choice(int(test_NA_test.shape[0]),nNAs_test)\n",
    "\n",
    "#Create a random index for the columns taking values only in the ones selected in the first place with dimension the number of na neccesary\n",
    "index_cNA_train=np.random.choice(index_column_NA,nNAs_train)\n",
    "index_cNA_test=np.random.choice(index_column_NA,nNAs_test)\n",
    "\n",
    "#Impute na \n",
    "for n in range(index_cNA_train.shape[0]):\n",
    "    train_NA_train.iloc[index_rNA_train[n],index_cNA_train[n]]=np.nan\n",
    "\n",
    "for n in range(index_cNA_test.shape[0]):\n",
    "    test_NA_test.iloc[index_rNA_test[n],index_cNA_test[n]]=np.nan\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3990ddce",
   "metadata": {},
   "source": [
    "## Imputation methods\n",
    "\n",
    "Once we have the data with NAs we are going to compare different options to impute missing values and see which one is the best one.\n",
    "\n",
    "We are using knn algorithm. Therefore, we need to scale the data. As many oprations are needed, we are going to use pipelines to do this comparaissons much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "391b3fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2461042.1161643835\n",
      "3338346.256438356\n",
      "2482044.815342466\n",
      "3317272.9808219178\n"
     ]
    }
   ],
   "source": [
    "#Split NA dataset into train and train validation\n",
    "train_NA_split=train_NA_train.iloc[:3650,:]\n",
    "train_NA_validation=train_NA_train.iloc[3650:,:]\n",
    "\n",
    "imputerMean=SimpleImputer(strategy=\"mean\")\n",
    "imputerMedian=SimpleImputer(strategy=\"median\")\n",
    "\n",
    "scaler_MinMax=MinMaxScaler()\n",
    "scaler_Robust=RobustScaler()\n",
    "\n",
    "knn=KNeighborsRegressor()\n",
    "\n",
    "clf=Pipeline([\n",
    "    (\"scale\",scaler_MinMax),\n",
    "    (\"impute\",imputerMean),\n",
    "    (\"knn\",knn)\n",
    "    ])\n",
    "\n",
    "clf2=Pipeline([\n",
    "    (\"scale\",scaler_Robust),\n",
    "    (\"impute\",imputerMean),\n",
    "    (\"knn\",knn)\n",
    "    ])\n",
    "\n",
    "clf3=Pipeline([\n",
    "    (\"scale\",scaler_MinMax),\n",
    "    (\"impute\",imputerMedian),\n",
    "    (\"knn\",knn)\n",
    "    ])\n",
    "\n",
    "clf4=Pipeline([\n",
    "    (\"scale\",scaler_Robust),\n",
    "    (\"impute\",imputerMedian),\n",
    "    (\"knn\",knn)\n",
    "    ])\n",
    "\n",
    "#1st Pipe Scale MinMax, impute mean  **BEST ONE**\n",
    "clf.fit(train_NA_split,train_target_split)\n",
    "train_validation_imp1=clf.predict(train_NA_validation)\n",
    "print(metrics.mean_absolute_error(train_validation_imp1,train_target_validation))\n",
    "\n",
    "#2nd Pipe Scale Robust, impute mean\n",
    "clf2.fit(train_NA_split,train_target_split)\n",
    "train_validation_imp2=clf2.predict(train_NA_validation)\n",
    "print(metrics.mean_absolute_error(train_validation_imp2,train_target_validation))\n",
    "\n",
    "#3rd Pipe Scale MinMax, impute median\n",
    "clf3.fit(train_NA_split,train_target_split)\n",
    "train_validation_imp3=clf3.predict(train_NA_validation)\n",
    "print(metrics.mean_absolute_error(train_validation_imp3,train_target_validation))\n",
    "\n",
    "#4th Pipe Scale Robust, impute median\n",
    "clf4.fit(train_NA_split,train_target_split)\n",
    "train_validation_imp4=clf4.predict(train_NA_validation)\n",
    "print(metrics.mean_absolute_error(train_validation_imp4,train_target_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce196fae",
   "metadata": {},
   "source": [
    "As we can see, depending on which scale method we use, we obtain similar results. When it comes to the imputation method the differences obtained are not very large when using the mean or the median value.\n",
    "\n",
    "Hence, we can say that withing a small difference, the best combination is the first one, where the missing values are imputed following the mean value and the scalation is given by the MinMax algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5bdfcd",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "In this step we are going to proceed with feature selection. We are going to use two different methods. In the first case we are going to use SelectKbest and later we will use PCA. In both of this different methods we are going to see how the dimension of the dataset can be reduced without loosing much key information for our predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95f92654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 250 candidates, totalling 250 fits\n",
      "{'knn__n_neighbors': 16, 'select__k': 269}\n",
      "2088806.3393835616\n"
     ]
    }
   ],
   "source": [
    "#Hyp tunning for knn\n",
    "selection=SelectKBest(f_regression)\n",
    "\n",
    "pipe1=Pipeline([\n",
    "    (\"scale\",scaler_MinMax),\n",
    "    (\"impute\",imputerMean),\n",
    "    (\"select\",selection),\n",
    "    (\"knn\",knn)\n",
    "    ])\n",
    "\n",
    "param_grid={\n",
    "    \"select__k\":[int(x) for x in np.linspace(start = 1, stop = 300, num = 50)],\n",
    "    \"knn__n_neighbors\":[2,4,8,16,32]\n",
    "    }\n",
    "\n",
    "train_cv_index=np.zeros(train_NA_train.shape[0]) \n",
    "train_cv_index[:3650] = -1\n",
    "train_cv_index = PredefinedSplit(train_cv_index)\n",
    "\n",
    "grid_tunning=GridSearchCV(pipe1,param_grid,\n",
    "                          scoring=\"neg_mean_squared_error\",cv=train_cv_index,n_jobs=-1,verbose=1)\n",
    "\n",
    "grid_tunning.fit(train_NA_train,train_target)\n",
    "print(grid_tunning.best_params_)\n",
    "bestparams=grid_tunning.best_params_\n",
    "\n",
    "pipe1.set_params(**bestparams)\n",
    "pipe1.fit(train_NA_train,train_target)\n",
    "\n",
    "model_select=pipe1.predict(train_NA_validation)\n",
    "print(metrics.mean_absolute_error(model_select,train_target_validation))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae2edcd",
   "metadata": {},
   "source": [
    "As we can see, the best model in this case contains 269 columns and is evaluated using k=16. Which is indeed a very large number of columms. Therefore, the dimension compared to the original one have not been reduced a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2ca6dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 250 candidates, totalling 250 fits\n",
      "{'knn__n_neighbors': 32, 'pca__n_components': 13}\n",
      "2160931.23390411\n"
     ]
    }
   ],
   "source": [
    "#PCA\n",
    "pca=PCA()\n",
    "\n",
    "pipe2=Pipeline([\n",
    "    (\"scale\",scaler_MinMax),\n",
    "    (\"impute\",imputerMean),\n",
    "    (\"pca\",pca),\n",
    "    (\"knn\",knn)\n",
    "    ])\n",
    "\n",
    "param_grid2={\n",
    "    \"pca__n_components\":[int(x) for x in np.linspace(start = 1, stop = 300, num = 50)],\n",
    "    \"knn__n_neighbors\":[2,4,8,16,32]\n",
    "    }\n",
    "\n",
    "\n",
    "grid_tunning2=GridSearchCV(pipe2,param_grid2,\n",
    "                          scoring=\"neg_mean_squared_error\",cv=train_cv_index,n_jobs=-1,verbose=1)\n",
    "\n",
    "grid_tunning2.fit(train_NA_train,train_target)\n",
    "print(grid_tunning2.best_params_)\n",
    "\n",
    "model_PCA=grid_tunning2.predict(train_NA_validation)\n",
    "print(metrics.mean_absolute_error(model_PCA,train_target_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3db021",
   "metadata": {},
   "source": [
    "For this case, we can see that the optimal model uses the first 13 principal components and the number of neighbors k=32, which is twice the number used in the first method.\n",
    "\n",
    "In constrast with the dimension obtained in the first method, using PCA we have significantly reduced it.\n",
    "\n",
    "The result obtained is sligtly worse than the one without PCA. However, it is impresive considering that this model uses much less information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4763385",
   "metadata": {},
   "source": [
    "## Extract names of attributes selected\n",
    "\n",
    "In order to see which attributes seems to be more important for the imputation method we can get the scores for each one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74ec7153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Columns</th>\n",
       "      <th>Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dswrf_s5_1</td>\n",
       "      <td>14430.933955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>dswrf_s5_4</td>\n",
       "      <td>13735.971184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dswrf_s3_1</td>\n",
       "      <td>12958.454520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dswrf_s4_1</td>\n",
       "      <td>12878.727974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>dswrf_s3_3</td>\n",
       "      <td>12809.606185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>pres_ms4_3</td>\n",
       "      <td>262.235729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>pwat_ea1_1</td>\n",
       "      <td>254.199299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>pres_ms3_3</td>\n",
       "      <td>250.117787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>apcp_sf2_2</td>\n",
       "      <td>249.846276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>pwat_ea2_1</td>\n",
       "      <td>245.658164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>269 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Columns        Scores\n",
       "11   dswrf_s5_1  14430.933955\n",
       "214  dswrf_s5_4  13735.971184\n",
       "9    dswrf_s3_1  12958.454520\n",
       "10   dswrf_s4_1  12878.727974\n",
       "142  dswrf_s3_3  12809.606185\n",
       "..          ...           ...\n",
       "146  pres_ms4_3    262.235729\n",
       "13   pwat_ea1_1    254.199299\n",
       "145  pres_ms3_3    250.117787\n",
       "68   apcp_sf2_2    249.846276\n",
       "14   pwat_ea2_1    245.658164\n",
       "\n",
       "[269 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=pipe1[\"select\"]\n",
    "columns_bool=a.get_support()\n",
    "a.scores_\n",
    "\n",
    "train_NA_train.columns[columns_bool].shape\n",
    "\n",
    "col_importance=pd.DataFrame({\"Columns\":train_NA_train.columns[columns_bool],\n",
    "                             \"Scores\":a.scores_[columns_bool]})\n",
    "\n",
    "col_importance.sort_values(by=\"Scores\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c36b31",
   "metadata": {},
   "source": [
    "As we can see in the output, the first columns with higher scores are the ones corresponding to the Downward surface shortwave Flux, refers to the radioactive energy in the wavelength interval 0.3 $\\mu m$, 4.0 $\\mu m$.\n",
    "\n",
    "In contrast the lower scores are given to variables like pwat and apcp, which stands for the  amount of water potentially available in the atmosphere for precipitation and 3-hour accumulated precipitation respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9973b546",
   "metadata": {},
   "source": [
    "## Evaluate best model\n",
    "\n",
    "Using the scores obtained on the validation data for the PCA model and the one using SelectKBest, we got that the best one was the SelectKBest. Eventhough the difference was not very large, the imputation with the optimal number of column gave better results than the PCA one also optimized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc9f7574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2161832.374829468\n"
     ]
    }
   ],
   "source": [
    "model1=grid_tunning.predict(test_NA_test)\n",
    "print(metrics.mean_absolute_error(model1,test_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8b0729",
   "metadata": {},
   "source": [
    "The best result obtained in the previous task was 2312173. We used the knn method optimazed with k=7 which was the higher value in the interval given to do the hyperparameter tunning. \n",
    "\n",
    "In this scenario we can see that we have obtained better results using another knn model, in this case with k=16 and selecting only the most important features of the data. \n",
    "\n",
    "From a logic point of view, the error obtained in this model should be higher than the one used in the first task due to the information lost while for using the selectKbest() function. Nevertheless, as it can bee seen, this  is not the case and we have obtained better results using this model.\n",
    "\n",
    "The fact that this method behavies better could be given by the fact that we are now only using k=16 for the knn algorithm or that the columns from the dataset that we are not considering in the model give information that penalizes the model instead of helping them to get better predictions. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
